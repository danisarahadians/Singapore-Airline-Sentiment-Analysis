{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/danisarahadians/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danisarahadians/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danisarahadians/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/danisarahadians/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Library Load Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "\n",
    "# Library Pre-Processing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#download library\n",
    "nltk.download('punkt') #punctuation package\n",
    "nltk.download('stopwords') #stopwords package\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "#warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization (TextVe  (None, 1149)              0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 1149, 128)         1720704   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 1149, 64)          41216     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1149, 64)          0         \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 1149, 32)          10368     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1149, 32)          0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 16)                2624      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1774963 (6.77 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 1774963 (6.77 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "loaded_model = tf.keras.models.load_model('model_lstm_2.keras')\n",
    "\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada tahapan ini saya melakukan hal yang sama pada data seperti handle stopwords, case folding, remove punctuation dll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menambahkan stopwords\n",
    "add_stopwords = ['to', 'I','the','a','my','and','i', 'you', 'is', 'for', 'in', 'of',\n",
    " 'it', 'on', 'have', 'that', 'me', 'so', 'with', 'be', 'but',\n",
    " 'at', 'was', 'just', 'I`m', 'not', 'get', 'all', 'this', 'are',\n",
    " 'out', 'like', 'day', 'up', 'go', 'your', 'good', 'got', 'from',\n",
    " 'do', 'going', 'no', 'now',  'work', 'will', 'about',\n",
    " 'one', 'really', 'it`s', 'u', 'don`t', 'some', 'know', 'see', 'can',\n",
    " 'too', 'had', 'am', 'back', '&', 'time', 'what', 'its', 'want', 'we',\n",
    " 'new', 'as', 'im', 'think', 'can`t', 'if', 'when', 'an', 'more',\n",
    " 'still', 'today', 'miss', 'has', 'they', 'much', 'there', 'last',\n",
    " 'need', 'My', 'how', 'been', 'home', 'lol', 'off', 'Just', 'feel',\n",
    " 'night', 'i`m', 'her', 'would', 'The', 'sq', 've', 'le', 'hr', 'pre', 'ca', 'th', 'b', 'uk', 'sat', 'sg', 'k', 'sfo', 'usb', 'cm', 'san', 'ft', 'hkg', 'veg', 'usd', 'rd', 'la',\n",
    "'bid', 'tag', 'usa', 'york', 'rang', 'ho', 'yr', 'fuss', 'los', 'bne', 'nap', 'hang', 'tad', 'wi', 'fi', 'sum', 'de', 'dp', 'fro', 'koh', 'samui', 'ana', 'sogi', 'prem',\n",
    "'aud', 'act', 'mnl', 'ie', 'klm', 'comb', 'ny', 'opt', 'au', 'en', 'eg', 'hub', 'yo', 'hung','sia', 'hop', 'com', 'nov', 'bc', 'cairn', 'dim', 'oct', 'bom', 'dhaka', 'cdg', 'nrt', 'cph' , 'na', 'log', 'kul', 'lh', 'w', 'ive', 'qf', 'shoe', 'tap', 'jam', 'lip', 'wan', 'oz', 'tho', 'siem', 'r', 'eve', 'melb', 'da', 'haha', 'airnz', 'coz', 'akl', 'utmost', 'gourmet', 'apps', 'mb', 'cx',  'dom', 'inr', 'pls', 'yum', 'bang', 'haagen', 'kix', 'sep', 'phee', 'rip', 'hip', 'un', 'warn', 'wee', 'z', 'ek', 'pic', 'sm', 'xmas', 'davao', 'penh', 'pcr', 'krug', 'pill', 'mar', 'ml', 'omg', 'def', 'jnb', 'kathmandu', 'pnr', 'emma', 'pudong', 'yangon', 'nang', 'qr', 'lol', 'ff', 'soo', 'so', 'vip', 'mai', 'ala', 'dxb', 'in', 'dme', 'pram', 'era', 'sim', 'bug', 'chan', 'bump', 'bent', 'pea', 'leo', 'sgn', 'amp', 'ed', 'ptv', 'dazs', 'dull', 'thr', 'aft','al', 'mad', 'pan', 'eu', 'mere', 'icing', 'danang', 'vn', 'bcn', 'singapur', 'guru', 'abit', 'fukuoka', 'wa', 'eau' , 'hoon', 'nicole', 'ham', 'ifs', 'perrier', 'sevice', 'convince', 'ref', 'easyjet', 'zrh', 'fond', 'ldn', 'ons', 'dire', 'hcmc', 'fr', 'toe', 'pond', 'ur', 'afghan', 'shenzen', 'hv', 'hkd', 'offs', 'icn', 'q', 'gaulle', 'uae', 'sooo', 'si', 'chianti', 'bengaluru', 'yeah', 'gps', 'nine', 'inc', 'jhb', 'madam', 'ban', 'signage', 'cheng', 'twg', 'alway', 'arn', 'swivel', 'krisshop', 'ya', 'ma', 'swa', 'chc', 'hyd', 'peculiar', 'oj', 'osl', 'prop', 'rhapsody', 'iam', 'wong', 'doona', 'gst', 'concoction', 'nj', 'doughy', 'fav', 'hum', 'stern', 'revamp', 'nzd', 'blunt', 'gon', 'int', 'bout', 'bento', 'hnd', 'ingham', 'bwn', 'cuz', 'jkt', 'yang', 'dr', 'mass', 'snag', 'piss', 'irate', 'adl', 'gel', 'econony', 'adjoining', 'rattle', 'chor', 'hide', 'hkt', 'amex', 'kim', 'goreng', 'singapre', 'ling', 'ap', 'damp', 'gastro', 'boss', 'temp', 'midst', 'gatwick', 'slop', 'krabi', 'sh', 'vi', 'ha', 'cmb', 'bak', 'inn', 'ful', 'ion','tbh', 'basinet', 'cab', 'andrea', 'welfare', 'kochi', 'lump', 'ashton', 'yatra', 'wotif', 'ent', 'an', 'ca', 'sang', 'ply', 'snug', 'rt', 'tongs', 'allways', 'grub', 'reckon', 'can', 'pr', 'ovo', 'maa', 'koi', 'sharifah', 'ab', 'bogus', 'nigh', 'sn', 'kat', 'david', 'john', 'savvy', 'muesli', 'ind', 'skywalk', 'imo', 'sqs', 'ng', 'teng', 'brat', 'mle', 'lye', 'iata', 'kee', 'spinal', 'hmmmm','yep', 'shin', 'gaffa' , 'chai', 'med', 'coccyx', 'eur', 'jean', 'agian', 'mee', 'kapoor', 'fog', 'sebastian', 'lingus', 'nhat', 'li', 'qi', 'saga', 'tsa', 'hagen', 'jasmine', 'ah', 'chunk', 'kebaya', 'fot', 'poc', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords defined\n",
    "stpwds_eng = list(set(stopwords.words('english')))\n",
    "stpwds_eng.append(add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A Function for Text Preprocessing\n",
    "\n",
    "def text_preprocessing(text):\n",
    "  # Case folding\n",
    "  text = text.lower()\n",
    "\n",
    "  # Mention removal\n",
    "  text = re.sub(\"@[A-Za-z0-9_]+\", \" \", text)\n",
    "\n",
    "  # Hashtags removal\n",
    "  text = re.sub(\"#[A-Za-z0-9_]+\", \" \", text)\n",
    "\n",
    "  # Newline removal (\\n)\n",
    "  text = re.sub(r\"\\\\n\", \" \", text)\n",
    "\n",
    "  # Remove '\n",
    "  text = re.sub(r\"'s\\b\",\"\", text)\n",
    "\n",
    "  # Remove Numbers\n",
    "  text = re.sub(r\"\\d+\", \" \", text)\n",
    "\n",
    "  # Remove Punctuation\n",
    "  text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "  # Whitespace removal\n",
    "  text = text.strip()\n",
    "\n",
    "  # URL removal\n",
    "  text = re.sub(r\"http\\S+\", \" \", text)\n",
    "  text = re.sub(r\"www.\\S+\", \" \", text)\n",
    "\n",
    "  # Delete word yang kemuculannya hanya 2 kata\n",
    "  text = re.sub(r\"\\b\\w{1,2}\\b\", \" \", text)\n",
    "\n",
    "  # Non-letter removal (such as emoticon, symbol (like μ, $, 兀), etc\n",
    "  text = re.sub(\"[^A-Za-z\\s']\", \" \", text)\n",
    "\n",
    "  # Skema: token dulu/split word -> stopwords\n",
    "  # Tokenization\n",
    "  tokens = word_tokenize(text)\n",
    "\n",
    "  # Stopwords removal\n",
    "  tokens = [word for word in tokens if word not in stpwds_eng]\n",
    "\n",
    "  # Lemmatization\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "  # Combining Tokens\n",
    "  text = ' '.join(tokens)\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **New Data Frame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat dataframe baru dengan kata yang akan di prediksi. Karena model kurang dalam memahami netral, jadi saya coba mau masukkin kata yang netral untuk mencoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>riview</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Facility is so bad and i dont like it, but...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              riview\n",
       "0  The Facility is so bad and i dont like it, but..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'riview' : 'The Facility is so bad and i dont like it, but the movie in plane was so fine'                                \n",
    "}\n",
    "\n",
    "data = pd.DataFrame([data])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>riview</th>\n",
       "      <th>riview_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Facility is so bad and i dont like it, but...</td>\n",
       "      <td>facility bad dont like movie plane fine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              riview  \\\n",
       "0  The Facility is so bad and i dont like it, but...   \n",
       "\n",
       "                          riview_processed  \n",
       "0  facility bad dont like movie plane fine  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing Data Inference\n",
    "data['riview_processed'] = data['riview'].apply(lambda arg: text_preprocessing(arg))\n",
    "\n",
    "#print\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat fungsi untuk return result prediksi\n",
    "def result_prediction(predictions):\n",
    "    '''\n",
    "    Fungsi ini bertujuan untuk menampilkan hasil dari prediksi sesuai label jika 0=negative, 1=netral, 2=positive\n",
    "    '''\n",
    "    if predictions[0] == 0:\n",
    "        result = 'Negative'\n",
    "    elif predictions[0] == 1:\n",
    "        result = 'Netral'\n",
    "    else:\n",
    "        result = 'Positive'\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predict**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Netral\n"
     ]
    }
   ],
   "source": [
    "# Prediksi\n",
    "predictions = np.argmax(loaded_model.predict(data['riview_processed']), axis=-1)\n",
    "\n",
    "# Call the function with the predictions\n",
    "result = result_prediction(predictions)\n",
    "\n",
    "# Print Result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil inferrence dari kata yang dimasukkan di data frame baru, hasilnya adalah benar terprediksi netral, karena dimulai dari negative riview lalu ditambahkan positive riview."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
